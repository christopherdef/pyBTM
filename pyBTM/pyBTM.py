import sys
import json
import os
from subprocess import call, Popen, PIPE, check_output
import shlex
import pathlib
from datetime import datetime
import itertools as it
from collections import Counter
import gensim
from gensim.models import CoherenceModel
from shutil import rmtree
from pyBTM import Logger

class BTM:
    '''
    Python class wrapper for running BTM
    Runs BTM code from github.com/xiaohuiyan/BTM
    Data at input_path should be plain text, with each line being a new document

    @param K -- number of topics
    @param alpha -- parameter for Dirichlet prior of P(z)
    @param beta -- parameter Dirichlet prior of P(w|z)
    @param niter -- number of learning iterations
    @param save_step -- steps before saving results

    @param input_path -- path to the data

    @param verbose -- include original btm script output
    @param clean_on_del -- deletes all files generated by this model when its
                           destructor is called (either by del or exit())

    NOTE: you usually don't wanna change these
    @param btm_bin_dir -- source for the btm binaries
    @param out_dir -- output for raw model data
    '''
    def __init__(self, K, alpha=1, beta=0.01, niter=50, save_step=501,
                 input_path=None, btm_bin_dir=None, out_dir=None,
                 verbose=False, clean_on_del=False):

        if input_path is None:
            raise Exception('please supply a path to the data via input_path parameter')

        logger = Logger(verbose)
        self.log = logger.log

        self.K          = K
        self.alpha      = alpha
        self.beta       = beta
        self.niter      = niter
        self.save_step  = save_step
        self.params     = {
                            'K':self.K, 'alpha':self.alpha,
                            'beta':self.beta, 'niter':self.niter,
                            'save_step':self.save_step
                          }

        self.input_path = input_path
        self.home_dir   = os.path.dirname(__file__)+'/'

        self.clean_on_del = clean_on_del
        self.verbose      = verbose

        # TODO: make visible in __repr__ and __str__
        self.param_str = f'k{self.K}_n{self.niter}_a%g_b%g' % (self.alpha, self.beta)

        self.log('='*10, f'BTM INITIALIZE ({self.param_str})', '='*10)
        self.log(f"K {self.K} | alpha {self.alpha} | beta {self.beta} | niter {self.niter} | input {self.input_path}")

        # locate or create important directories
        input_fname = os.path.basename(self.input_path)
        input_fname = input_fname[:input_fname.rfind('.')]

        self.bin_dir = btm_bin_dir or f'{self.home_dir}BTM/'
        self.output_dir = out_dir or f'{self.home_dir}output/{input_fname}_output/'

        self.model_dir=f'{self.output_dir}model/{self.param_str}/'
        pathlib.Path(self.model_dir).mkdir(parents=True, exist_ok=True)

        # the input docs for training
        self.doc_pt=f'{self.input_path}'

        # TODO: START - duplicated code, very bad
        # output model docs
        self.pz_pt  = f'{self.model_dir}{self.param_str}.pz'
        self.pwz_pt = f'{self.model_dir}{self.param_str}.pw_z'
        self.pzd_pt = f'{self.model_dir}{self.param_str}.pz_d'

        # vocab and indexing docs
        self.voca_pt=f'{self.output_dir}voca.txt'
        self.dwid_pt=f'{self.output_dir}doc_wids.txt'
        # TODO: END

        # set progress flags
        self.indexing_complete = os.path.exists(self.voca_pt) and os.path.exists(self.dwid_pt)
        self.learning_complete = os.path.exists(self.pz_pt) and os.path.exists(self.pwz_pt)
        self.inference_complete = os.path.exists(self.pzd_pt)

        if self.indexing_complete:
            self._load_wc()

    def info(self):
        '''
        Returns large string with information about the current pyBTM object
        '''

        status_str = lambda b : "complete" if b else "incomplete"
        info_str = f'''
        PARAMETERS:
        K={self.K} / alpha={self.alpha} / beta={self.beta}
        niter={self.niter} / save_step={self.save_step}

        STATUS:
        indexing - {status_str(self.indexing_complete)}
        learning - {status_str(self.learning_complete)}
        inference - {status_str(self.inference_complete)}

        PATHS:
        input={os.path.realpath(self.input_path)}
        bin={os.path.realpath(self.bin_dir)}
        out={os.path.realpath(self.output_dir)}
        pz={os.path.realpath(self.pz_pt)}
        pwz={os.path.realpath(self.pwz_pt)}
        pzd={os.path.realpath(self.pzd_pt)}
        voca={os.path.realpath(self.voca_pt)}
        dwids={os.path.realpath(self.dwid_pt)}
        '''.replace('\t', '')

        return info_str

    def __del__(self):
        if self.clean_on_del:
            self.clean()


    def clean(self):
        '''
        Clean up model files
        '''
        rmtree(self.output_dir)


    def index_documents(self, force=False):
        '''
        Assign all words a numeric id (doc_wids) and index words in each document (vocab)

        @param force -- runs indexing even if indexing has completed
        '''
        self.log(f"========== Index Docs ({self.param_str}) ==========")

        # docs after indexing
        self.dwid_pt=f'{self.output_dir}doc_wids.txt'

        # vocabulary file
        self.voca_pt=f'{self.output_dir}voca.txt'

        # we'll never want to index all the documents twice
        if os.path.exists(self.voca_pt) and os.path.exists(self.dwid_pt) and not force:
            self.log('indexing already complete')
            self.indexing_complete = True

        # run indexing script
        else:
            index_cmd = shlex.split(f'python {self.home_dir}indexDocs.py {self.doc_pt} {self.dwid_pt} {self.voca_pt}')
            self.log(' '.join(index_cmd))
            p = Popen(index_cmd, stdout=PIPE)
            self.log(p.communicate()[0].decode()) # kinda crude way to pipe subprocess output back to caller

            self.indexing_complete = True

        self._load_wc()
        self.log('indexing complete')
        return

    def learn_topics(self, force=False):
        '''
        Learn parameters p(z) and p(w|z)

        @param force -- runs learning even if learninghas completed
        '''
        if not self.indexing_complete:
            raise Exception('must index docs with BTM.index_documents() before learning parameters')

        self.log(f"========== Topic Learning ({self.param_str}) ==========")

        # might want to learn topics multiple times
        if self.learning_complete and not force:
            self.log('learning already complete')
            return

        W = self._count_file_lines(self.voca_pt) # vocabulary size

        # run btm est
        btm_est_cmd = shlex.split(f'{self.bin_dir}btm est '+\
                                  f'{self.K} {W} {self.alpha} {self.beta} {self.niter} {self.save_step} '+\
                                  f'{self.dwid_pt} {self.model_dir}')
        self.log(' '.join(btm_est_cmd))
        p = Popen(btm_est_cmd, stdout=PIPE)
        self.log(p.communicate()[0].decode())

        self.learning_complete = True
        self.log('learning complete')
        return

    def infer_documents(self, force=False):
        '''
        Infer p(z|d) for each doc

        @param force -- runs inference even if inference has completed
        '''
        if not self.indexing_complete or not self.learning_complete:
            raise Exception('must index documents, then learn models before inferring topic distribution per document\n'+\
                            'run .index_documents() and .learn_topics() before inference')

        # might want to run inference multiple times
        if self.inference_complete and not force:
            self.log('inference already complete')
            return

        self.log(f"========== Infer P(z|d) ({self.param_str}) ==========")

        btm_inf_cmd = shlex.split(f'{self.bin_dir}btm inf sum_b '+\
                                  f'{self.K} {self.alpha} {self.beta} {self.niter} '+\
                                  f'{self.dwid_pt} {self.model_dir}')
        self.log(' '.join(btm_inf_cmd))

        p = Popen(btm_inf_cmd, stdout=PIPE)
        self.log(p.communicate()[0].decode())

        self.inference_complete = True
        self.log('inference complete')
        return

    def get_topics(self, L=10, use_words=True, include_likelihood=True):
        '''
        Connects the topic distribution (pz) with the word-by-topic distribution (pw_z) and the vocab dict
        and returns a 3D array where each row is a topic and each entry is a [word, word_likelihood] pair
        If include_likelihood is false, returns a 2D array where each row is a topic and each entry is a word

        @param L -- number of words per topic to return in array; L<=0 --> return all words in topic
        @param use_words -- if False, shows only the word_id in the array
        @param include_likelihood -- if False, each entry returned is a word rather than a [word, word_likelihood] pair
        @param lazy -- if True, each entry returned is a generator rather than full array

        @return 3D array of top wids/words per topic along paired with those words' likelihoods
        '''
        vocab = self.get_vocab()
        pz = self.get_pz()
        pw_z = self.get_pwz(lazy=True)

        i = 0
        top_words_per_topic = {}
        for topic_line in pw_z:
            z = pz[i]

            # sort the w|z values
            sorted_wz = sorted(enumerate(topic_line), key=lambda wid_wz : float(wid_wz[1]), reverse=True)
            top_wz = sorted_wz

            # slice if needed
            if L > 0:
                top_wz = sorted_wz[:L]

            # TODO: messy, hard to read, pls fix
            # apply parameters use_words and include_likelihood to the top words in topic z
            apply_use_words = lambda wid : str(wid) if not use_words else vocab[str(wid)]
            apply_include_likelihood = lambda wid_wz_pair : wid_wz_pair if include_likelihood else wid_wz_pair[0]
            top_wz = list([\
                           apply_include_likelihood([\
                                                     apply_use_words(wid),\
                                                     str(wz)\
                                                    ])
                           for wid, wz in top_wz])

            top_words_per_topic[z] = top_wz
            i+=1

        # returns array of top words per topic along with those words' likelihoods
        return top_words_per_topic

    def get_pz(self):
        '''
        Return K x 1 array P(z)

        K = number of topics
        '''
        if not self.learning_complete:
            raise Exception('topic learning has not been completed')

        pz = open(self.pz_pt, 'r').read().rstrip().split(' ')

        return pz

    def get_pwz(self, lazy=True, dtype=str):
        '''
        Return K x V array P(w|z)

        K = number of topics
        V = size of vocabulary
        '''
        if not self.learning_complete:
            raise Exception('topic learning has not been completed')
        pwz_file = open(self.pwz_pt, 'r')
        process_pwz_line = lambda line : list(dtype(e) for e in line.rstrip().split(' '))

        if lazy:
            return self._lazy_load(pwz_file, process_pwz_line)
        else:
            pwz = []
            for topic_line in pwz_file:
                word_likelihood = process_pwz_line(topic_line)
                pwz.append(word_likelihood)

            return pwz

    # TODO: implement lazy
    def get_pzd(self, lazy=True):
        '''
        Return N x K array P(z|d)

        K = number of topics
        N = number of documents
        '''
        if not self.inference_complete:
            raise Exception('inference has not been completed')

        pzd_file = open(self.pzd_pt, 'r')

        if lazy:
            process_pzd_line = lambda line : line.rstrip().split(' ')
            return self._lazy_load(pzd_file, process_pzd_line)

        pzd = []
        for doc_line in pzd_file:
            topic_likelihood = doc_line.rstrip().split(' ')
            pzd.append(topic_likelihood)

        return pzd


    def get_word_count(self):
        '''
        Returns size of vocabulary (V)
        Must index before calling
        '''
        if not self.indexing_complete:
            raise Exception('indexing has not been completed')

        return self.word_count

    def get_doc_count(self):
        '''
        Returns number of documents (N)
        Must index before calling
        '''

        if not self.indexing_complete:
            raise Exception('indexing has not been completed')

        return self.doc_count

    # TODO implement lazy
    def get_dwids(self, lazy=False):
        '''
        Returns bag-of-word representation of the documents
        Must index before calling
        '''
        if not self.indexing_complete:
            raise Exception('indexing has not been completed')

        self.log('unpacking doc_wids')
        return list(l.strip().split(' ') for l in open(self.dwid_pt, 'r'))

    def get_word_freq(self, bow_docs=None):
        '''
        Returns the frequency of each word in a Counter dictionary
        Must index before calling

        @param bow_docs -- bag-of-word representation of the documents (default self.get_dwids())
        '''
        if not self.indexing_complete:
            raise Exception('indexing has not been completed')

        if bow_docs is None:
            bow_docs = self.get_dwids()
        self.log('generating word frequency distribution')
        return Counter(it.chain(*bow_docs))

    def get_vocab(self):
        '''
        Returns dictionary of the full vocabulary
        Maps words to word-ids
        Must index before calling
        '''
        if not self.indexing_complete:
            raise Exception('indexing has not been completed')

        self.log('unpacking vocab')
        return dict((tuple([wid_w[0], wid_w[1]]) for wid_w in\
                          [line.strip().split('\t') for line in open(self.voca_pt, 'r')]))


    # TODO: implement coherence stuff
    def build_coherence_model(self, measures=['c_v']):
        '''
        Build gensim coherence models after inference has been complete
        Must run inference before calling

        @param measures -- coherence measure to model (default c_v)
        '''
        if not self.inference_complete:
            raise Exception('inference has not been completed')

        self.log('Loading documents')
        docs = self.get_dwids() # documents are the same for each btm model

        self.log('Building gensim dictionary')
        gen_dict = gensim.corpora.Dictionary.from_documents(docs)

        coherence_dict = {}

        # load topics for the model
        self.log(f'Loading topics for {self.param_str}')
        gen_topics = self.get_topics(L=-1, use_words=False, include_likelihood=False).values()

        # evaluate coherence for each model and each measure
        for measure in measures:

            # build the model
            self.log(f'Building coherence model {measure} for {self.param_str}')
            cm = CoherenceModel(topics=gen_topics, dictionary=gen_dict, texts=docs, coherence=measure)

            # calculate coherence
            self.log(f'Calculating coherence {measure} value')
            cm_value = cm.get_coherence()

            # package results
            coherence_dict[measure] = (cm, cm_value)

        return coherence_dict

    # TODO: naive soln, integrate some memmap or dask stuff later
    def _lazy_load(self, f, process_line):
        '''
        Lazily load large file

        @param f -- file to load
        @param process_line -- function to run on each line loaded before yielding
        '''
        for line in f:
            yield process_line(line)


    def _count_file_lines(self, pt):
        '''
        Small helper for counting lines in a file

        @param pt -- path to target file
        '''
        try:
            b_out = check_output(shlex.split(f'wc -l {pt}'))
            if len(b_out) > 0:
                code = b_out.split(b' ')[0]
                return int(code)
        except Exception:
            print(f'Error calling wc: {bytes.decode(b_out, "utf8")}')

            # backup word counter, ~10x longer for big files
            import mmap
            f = open(pt, 'r+b')
            mm = mmap.mmap(f.fileno(), 0)

            count = 0
            i =  mm.find(b'\n')
            nb = mm.size()

            while i > 0 and i < mm.size():
                mm.seek(i+1)
                i = mm.find(b'\n')
                count += 1

            return count

    def _load_wc(self):
        '''
        Helper for loading word and document counts
        Used in init and index_documents
        '''
        self.doc_count  = self.N = self._count_file_lines(self.dwid_pt)
        self.word_count = self.V = self._count_file_lines(self.voca_pt)
